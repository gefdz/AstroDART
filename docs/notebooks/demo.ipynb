{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5024362-2e4b-4ed7-a7f5-ef68cf34b82f",
   "metadata": {},
   "source": [
    "# Reduction & Photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a21b48-aeb0-42ee-ac03-8cdf14416fb3",
   "metadata": {},
   "source": [
    "### First stage: Image reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed979346-e10a-4ac4-b718-4182cc2ebb73",
   "metadata": {},
   "source": [
    "This pipeline is intended for two main uses, astrophotography data reduction (including image combination and alignment) and photometry. This notebook in particular is a demo of the latter. This pipeline is basically a wrapper of astropy functions and in particular those of [ccdproc](https://ccdproc.readthedocs.io/en/latest/). We being by importing the necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e5b0e-300d-4a38-8823-2aeccb5fcce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import os as os\n",
    "from AstroDART.utils import data_organiser, get_directory, image_combination\n",
    "from AstroDART.first_stage import *\n",
    "from AstroDART.second_stage import *\n",
    "from AstroDART.third_stage_photometry import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaddefc-cb0a-4284-878c-55503b6f5f03",
   "metadata": {},
   "source": [
    "Let's define where the data are stored. I recommend using the empty directories you can find when you did the git clone of the repository but any uncal and reduced data directories will work. However it is important that the uncal_data_dir has the dates of observation as YYMMDD inside. 230313_crab will work if you observed the Crab Nebula that day, but it could generate conflict further down the line, the code allows for multiple targets to be observed in one night so there is no need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a2663-3769-45d8-b8b4-24d1d4b3f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_dir = '/uncal_data/'\n",
    "reduction_dir = '/reduced_data/'\n",
    "uncal_data_dir = '/uncal_data/XXXXX/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f004797-7776-4672-9e9a-7cf7b2737d68",
   "metadata": {},
   "source": [
    "The first functionality is related to fits headers. Although there are standards in place different telescopes use different keys for the same field in the fits header. For instance IAC80 uses INSFILTE for the filter key. However in the hopes of making the code more general we would like to use the word FILTER instead. We can change this key using the header tools class. This class also allows to append new keys and update the value of already existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73ecb0-9e6c-4ece-99d7-d173717580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_keys = {'INSFILTE': 'FILTER'}\n",
    "append_keys = {'BUNIT': 'adu', 'GAIN' : 4.23, 'RON' : 6.5, 'SCALE': 0.322}\n",
    "update_keys = {'OBJECT': 'ASAS'}\n",
    "\n",
    "\n",
    "uncal_files = glob.glob(uncal_data_dir + '*.fits')\n",
    "\n",
    "\n",
    "header_tools(files=uncal_files,header_dict=change_keys).change_header_key_name()\n",
    "\n",
    "header_tools(files=uncal_files,header_dict=update_keys).update_header_values()\n",
    "\n",
    "header_tools(files=uncal_files,header_dict=append_keys).append_header_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3dd09-5335-4848-95c5-25f57b7e9905",
   "metadata": {},
   "source": [
    "Now let us organize the data in the necessary folders. This will make things more easy to follow. The following code will copy the data to the directory where the reduction is to be done. This will only copy those files that have the target field in $data\\_ organiser$ equal to the name in the $OBJECT$ field in the header. This allows you to reduce targets observed in the same night independently. All commands expect lists of strings (the paths of the files), not the hdul data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d1f67-0451-488e-9720-9cdc345dbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "organiser = data_organiser(uncal_dir = uncal_dir, reduction_dir = reduction_dir,date='XXXXX',target='XXXX',overwrite='yes')\n",
    "organiser.run()\n",
    "\n",
    "data_dir = organiser.get_directory()\n",
    "\n",
    "\n",
    "bias_dir, bias_files = get_directory(data_dir).bias()\n",
    "flat_dir, flat_files, flat_filters = get_directory(data_dir).flatfield()\n",
    "object_dir, object_files, object_filters = get_directory(data_dir).science()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce87de6-b425-44ca-ad96-88cace8d8b5f",
   "metadata": {},
   "source": [
    "Now that we have the data where we want it let's start the reduction. First thing to do is correct the image of overscan and trim the image. I will skip overscan as it is not as common you may look into the source code but the idea is the same as the rest of commands. Assuming the image has some vignneting (or that we want to crop it) lets define a region and run the trimmer class. After that we will calculate the deviation (the uncertainties basically, these will be used later when doing photometry) and we will correct of gain. If this filed is not in the header you should run it using header tools as described above. We will start with the bias files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8a384-0ce7-4ec6-b8e8-1188357f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = '[1000:3040,1000:3040]'\n",
    "\n",
    "step = trimmer(wdir = bias_dir, files = bias_files,region=region, overwrite='yes')\n",
    "\n",
    "results_trim = step.run()\n",
    "\n",
    "step = deviation_calculation(wdir = bias_dir, files = results_trim, overwrite='yes')\n",
    "\n",
    "results_dc = step.run()\n",
    "\n",
    "step = gain_correction(wdir = bias_dir, files = results_dc,overwrite='yes')\n",
    "\n",
    "results_gc = step.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ccbb9-8819-4f4e-a841-3730233e20bf",
   "metadata": {},
   "source": [
    "The masterbias can be calculated with the $image\\_combination$ class. The methods for combining images are median, average, sum, scaled and weighted (please refer to [the ccdproc image combination guide](https://ccdproc.readthedocs.io/en/latest/image_combination.html) for more information). The methods for clipping method are sigclip, extrema_clipping and minmax. You may also specify the maximum amount of memory used, the deafult is 1 GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f14a8-8d03-4cb3-921b-bb8413f866c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterbias = image_combination(wdir=bias_dir,files=results_gc,output_name='Masterbias.fits',\n",
    "                               combining_method='median',clipping_method='sigclip',minclip=2,maxclip=5).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817e97a-0023-4242-a8d4-167ab62a0afe",
   "metadata": {},
   "source": [
    "Let's repeat the same steps but for flats, in this case we will subtract the masterbias file and combine the images into their respective masterflatfield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d90c6-716f-4e04-b6d6-556813ccd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(flat_dir)):\n",
    "\n",
    "    step = trimmer(wdir = flat_dir[ii], files = flat_files[ii], region=region, overwrite='yes')\n",
    "\n",
    "    results_trim = step.run()\n",
    "\n",
    "    step = deviation_calculation(wdir = flat_dir[ii], files = results_trim, overwrite='yes')\n",
    "\n",
    "    results_dc = step.run()\n",
    "\n",
    "    step = gain_correction(wdir = flat_dir[ii], files = results_dc,overwrite='yes')\n",
    "\n",
    "    results_gc = step.run()\n",
    "\n",
    "    results_bias_subtraction = subtract_bias(wdir=flat_dir[ii],files=results_gc,masterbias_file=masterbias,overwrite='yes').run()\n",
    "\n",
    "    globals()['masterflat_' + flat_filters[ii]] = image_combination(wdir=flat_dir[ii],\n",
    "                                                                    files=results_bias_subtraction,\n",
    "                                                                    output_name=f'Masterflat_{flat_filters[ii]}.fits',\n",
    "                                                                    combining_method='median',clipping_method='sigclip',\n",
    "                                                                    minclip=2,maxclip=5).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33a0e8-2df0-452e-8c0b-8130fb37af1e",
   "metadata": {},
   "source": [
    "Now let's do the same for the object files, we will divide by the master flatfield, there is no need to normalize as the function takes care of that by itself. We could also eliminate the cosmic rays, but given that we are interested in photometry we can skip that, it is shown in the astrophotography demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a032c0-4bbc-41ac-97ce-3810eb714ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(object_dir)):\n",
    "\n",
    "    step = trimmer(wdir = object_dir[ii], files = object_files[ii], region=region, overwrite='yes')\n",
    "\n",
    "    results_trim = step.run()\n",
    "\n",
    "    step = deviation_calculation(wdir = object_dir[ii], files = results_trim, overwrite='yes')\n",
    "\n",
    "    results_dc = step.run()\n",
    "\n",
    "    step = gain_correction(wdir = object_dir[ii], files = results_dc,overwrite='yes')\n",
    "\n",
    "    results_gc = step.run()\n",
    "\n",
    "    step = cosmic_ray_laplacian_correction(wdir = object_dir[ii], files = results_gc, overwrite='yes')\n",
    "\n",
    "    results_crlc = step.run()\n",
    "\n",
    "    results_bias_subtraction = subtract_bias(wdir=object_dir[ii],files=results_crlc,masterbias_file=masterbias,overwrite='yes').run()\n",
    "\n",
    "    results_flat_correction = correct_flatfield(wdir=object_dir[ii], files=results_bias_subtraction, \n",
    "                                                masterflatfield_file = globals()['masterflat_' + flat_filters[ii]], overwrite='yes').run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff8e29-6b42-45b2-8626-a5f2994a8876",
   "metadata": {},
   "source": [
    "Done! The design philosophy behind this pipeline is that you can either run it entirely with minimal user input (only at the beginning) or step by step. This generates a lot of intermediate files, and these may not be necessary at the end. So let's just delete them and finalize stage 1. The finalize_stage1 class returns the directories and files that have been reduced and rearranges the files so that you have date/target/filters/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4fb2c-50ea-4da8-a59d-b4517cc1bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories, files = finalize_stage1(data_dir=data_dir,object_dir=object_dir,bias_dir=bias_dir,flatfield_dir=flat_dir).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f44b4-2b5d-40b8-a52e-765dd65cb26d",
   "metadata": {},
   "source": [
    "### Second stage: Astrometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bae1d2-6930-46dc-aedf-735b65ae7757",
   "metadata": {},
   "source": [
    "The final objective is doing photometry, so, aligning images is one option to determine where the stars we are interested will be in all images. The photometry tools available in this program will work with pixel coordinates. Nevertheless given that images may be taken in different nights with slightly dfferent fields this could result in FOVs which are extremely cropped. Moreover I find that doing astrometry is a much more elegant way to find the positions of targets. This program makes use of [Astrometry.net](https://nova.astrometry.net/). You should register and the obtain your API key. It will be used to astrometrize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d237a-2abf-4a52-b132-2eee345643c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "fwhms_out = []\n",
    "for ii in range(len(directories)):\n",
    "    globals()[f'failed_astrometry_files_filter_{object_filters[ii]}'] = []\n",
    "    fwhms = get_fwhm(files[ii]).estimate_fwhm()\n",
    "    fwhms_out.append(fwhms)\n",
    "    print(fwhms)\n",
    "    globals()[f'failed_astrometry_files_filter_{object_filters[ii]}'].append(astrometry(directory=directories[ii],\n",
    "                                                                                        files=files[ii],api_key=api_key,fwhm=fwhms).run())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a7949-09d5-4248-8795-b8b2d27bbe2f",
   "metadata": {},
   "source": [
    "If you decided to stop here (it can take up to 2 hours in the worst case scenario depending on the astrometry) and continue another day but stll decided to continue with this code you would find that the lists of files etc are not stored in memory. Because doing it yourself is quite annoying the code has got you covered. Just run the following. Data dir should be where the data are, so /reduced_data/YYMMDD/target/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398dea7-3e80-44d2-9167-d5bbdeab664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories, files, filters = get_directory(data_dir = '/XXXX/XXXX/').science_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed21bd-1669-49e5-a531-2823aef4bba3",
   "metadata": {},
   "source": [
    "Now we are ready to look for targets. We will use the class coordinates for this. This class has 4 functions. \n",
    "\n",
    "First is from_image: this will plot the field given a reference frame, there you will be asked for a target (use the numbers that appear next to the stars). You will hit enter and then you will be asked for comparison stars (e.g. 2,3,8). This will return the skycoord coordinates of these objects inside lists.\n",
    "\n",
    "Second is from_index: if you already now your star has the brightness index 5 in your reference image there is no need to repeat the whole thing, just specify the indeces.\n",
    "\n",
    "Third and fourth are from_px_to_world and from_world_to_px. These are pretty self explanatory, you need to provide lists (even for only one target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30387bff-cd8e-488d-aea4-5daaf0c48d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "target, comparison = coordinates(files=files[0],reference_frame=0).from_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad701b-2df1-415b-ae3a-4f6b8a24dd04",
   "metadata": {},
   "source": [
    "If we want to convert these world coordinates to pixel coordinates we would do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d637b-02e9-4e82-af4f-cebd9b030b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_in_px = []\n",
    "comparison_in_px = []\n",
    "\n",
    "for ii in range(len(files)):\n",
    "\n",
    "    target_in_px_filter = coordinates(files=files[ii],world_coordinates=target).world_to_px()\n",
    "    target_in_px.append(target_in_px_filter) \n",
    "\n",
    "for ii in range(len(comparison)):\n",
    "    comparison_in_px_filters = []\n",
    "    for jj in range(len(files)):\n",
    "        comparison_in_px_filter = coordinates(files=files[jj],world_coordinates=comparison[ii]).world_to_px()\n",
    "        comparison_in_px_filters.append(comparison_in_px_filter) \n",
    "    \n",
    "    comparison_in_px.append(comparison_in_px_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7d761-f044-452b-914b-a81271627640",
   "metadata": {},
   "source": [
    "### Third stage: Photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d08ea9-7979-41dc-818a-653cde03c1fc",
   "metadata": {},
   "source": [
    "Photometry is where the user has to decide what to do the most. Let's go one step at a time. First the photometry method you will use; options are either sky or pixel, this refers to which type of coordinates will be used. The next is the aperture type which can be fwhm or custom. In the first case the fwhm of the image will be determined. The gist is that first sources are detected, the centroid that is, next the radial size of the fwhm is obtained as the average in each direction around the centroid where the flux drops by half. This is computed for all sources in one frame and the average of all of them is the FWHM. If selected the radius, annulus and dannulus will be 2.5, 3.5 and 4.5 times the FWHM. The other option is custom, in this case all these fields are specified by the user, in pixels or in arcseconds (both as float numbers). \n",
    "\n",
    "\n",
    "Because we are interested in doing science we want the time format to be in bjd_tdb (barycentric julian date in temps dynamique barycentrique) we need to know where the observations took place, to account for the time it took light to get to where we observed. The location can be specified as a string if it is a well know observatory, such as Observatorio del Teide, OT, or as a list with latitude, longitude and height, all floats in degrees and meters respectively.\n",
    "\n",
    "We can also do photometry of 1 single object using single coord, or a target and some comparisons using target_coord and comparison_coord\n",
    "\n",
    "Example of FWHM photometry using sky coordinates at Observatorio del Teide below.\n",
    "\n",
    "Note that it is necessary to specify filters used as well. If save_data is True an .h5 file will be generated containing all information (coords, timestamps in bjd_tdb, flux, flux_err, mag (with respect to a zero mag which is 25 by default) and mag_err. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261bd450-728e-478f-92dd-670967e512be",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = photometry(phot_method='sky',aperture_type='fwhm',files=files,filters=filters,obs_location='OT',\n",
    "                  target_coord = target, comparison_coord=comparison,save_results=True,save_data_dir='/XXXX/XXX/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9fb674-a188-49d5-85e7-c4a817755dad",
   "metadata": {},
   "source": [
    "If we wanted to do it with custom apertures with pixel photometry we could do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff38fbb-6bb9-4d8d-8fbc-2cbff35768e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = photometry(phot_method='pixel',aperture_type='custom',radius_phot=15,\n",
    "                  radius_annulus=20,radius_dannulus=25,files=files,\n",
    "                  filters=filters,obs_location='OT',target_coord = target_in_px,\n",
    "                  comparison_coord=comparison_in_px,\n",
    "                  save_results=True,save_data_dir='/XXXX/XXX/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c68538-79ff-4497-8124-8b97c143958d",
   "metadata": {},
   "source": [
    "Finally we would run the code, this will return a dictionary with the same information stored in the .h5 file, in case you want to quickly do some light curve analysis. For the multi_coord we would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad628b1-77e3-4bd0-9567-1afc1211b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_results = step.run_multi_coord()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebaf441-6586-400b-aa0a-26cf1293e4ee",
   "metadata": {},
   "source": [
    "Done!! This is the end of stage 3 and the photometry guide. You can now do real science!! I will upload exoplanet and eclipsing binary light curve analysis at a later time but in the mean time I recommend you look into PyTransit, Pylightcurve and Phoebe2 for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d663ffd-3840-4eda-bc83-7c4635c9cce6",
   "metadata": {},
   "source": [
    "# End!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

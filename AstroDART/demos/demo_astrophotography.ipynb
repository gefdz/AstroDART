{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5024362-2e4b-4ed7-a7f5-ef68cf34b82f",
   "metadata": {},
   "source": [
    "# Reduction & Astrophotography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a21b48-aeb0-42ee-ac03-8cdf14416fb3",
   "metadata": {},
   "source": [
    "### First stage: Image reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed979346-e10a-4ac4-b718-4182cc2ebb73",
   "metadata": {},
   "source": [
    "This pipeline is intended for two main uses, astrophotography data reduction (including image combination and alignment) and photometry. This notebook in particular is a demo of the former. This pipeline is basically a wrapper of astropy functions and in particular those of [ccdproc](https://ccdproc.readthedocs.io/en/latest/). We being by importing the necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e5b0e-300d-4a38-8823-2aeccb5fcce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import os as os\n",
    "from AstroDART.utils import  data_organiser, get_directory, image_combination\n",
    "from AstroDART.first_stage import *\n",
    "from AstroDART.second_stage import *\n",
    "from AstroDART.third_stage_astrophotography import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaddefc-cb0a-4284-878c-55503b6f5f03",
   "metadata": {},
   "source": [
    "Let's define where the data are stored. I recommend using the empty directories you can find when you did the git clone of the repository but any uncal and reduced data directories will work. However it is important that the uncal_data_dir has the dates of observation as YYMMDD inside. 230313_crab will work if you observed the Crab Nebula that day, but it could generate conflit further down the line, the code allows for multiple targets to be observed in one night so there is no need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a2663-3769-45d8-b8b4-24d1d4b3f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_dir = '/uncal_data/'\n",
    "reduction_dir = '/reduced_data/'\n",
    "uncal_data_dir = '/uncal_data/XXXXX/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f004797-7776-4672-9e9a-7cf7b2737d68",
   "metadata": {},
   "source": [
    "The first functionality is related to fits headers. Although there are standards in place different telescopes use different keys for the same field in the fits header. For instance IAC80 uses INSFILTE for the filter key. However in the hopes of making the code more general we would like to use the word FILTER instead. We can change this key using the header tools class. This class also allows to append new keys and update the value of already existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73ecb0-9e6c-4ece-99d7-d173717580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_keys = {'INSFILTE': 'FILTER'}\n",
    "append_keys = {'BUNIT': 'adu', 'GAIN' : 4.23, 'RON' : 6.5, 'SCALE': 0.322}\n",
    "update_keys = {'OBJECT': 'ASAS'}\n",
    "\n",
    "\n",
    "uncal_files = glob.glob(uncal_data_dir + '*.fits')\n",
    "\n",
    "\n",
    "header_tools(files=uncal_files,header_dict=change_keys).change_header_key_name()\n",
    "\n",
    "header_tools(files=uncal_files,header_dict=update_keys).update_header_values()\n",
    "\n",
    "header_tools(files=uncal_files,header_dict=append_keys).append_header_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3dd09-5335-4848-95c5-25f57b7e9905",
   "metadata": {},
   "source": [
    "Now let us organize the data in the necessary folders. This will make things more easy to follow. The following code will copy the data to the directory where the reduction is to be done. This will only copy those files that have the target field in $data\\_ organiser$ equal to the name in the $OBJECT$ field in the header. This allows you to reduce targets observed in the same night independently. All commands expect lists of strings (the paths of the files), not the hdul data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d1f67-0451-488e-9720-9cdc345dbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "organiser = data_organiser(uncal_dir = uncal_dir, reduction_dir = reduction_dir,date='XXXXX',target='XXXX',overwrite='yes')\n",
    "organiser.run()\n",
    "\n",
    "data_dir = organiser.get_directory()\n",
    "\n",
    "\n",
    "bias_dir, bias_files = get_directory(data_dir).bias()\n",
    "flat_dir, flat_files, flat_filters = get_directory(data_dir).flatfield()\n",
    "object_dir, object_files, object_filters = get_directory(data_dir).science()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce87de6-b425-44ca-ad96-88cace8d8b5f",
   "metadata": {},
   "source": [
    "Now that we have the data where we want it let's start the reduction. First thing to do is correct the image of overscan and trim the image. I will skip overscan as it is not as common you may look into the source code but the idea is the same as the rest of commands. Assuming the image has some vignneting (or that we want to crop it) lets define a region and run the trimmer class. After that we will calculate the deviation (the uncertainties basically, these are not strictly necessary but you can still do some science with these images) and we will correct of gain. If this filed is not in the header you should run it using header tools as described above. We will start with the bias files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8a384-0ce7-4ec6-b8e8-1188357f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = '[1000:3040,1000:3040]'\n",
    "\n",
    "step = trimmer(wdir = bias_dir, files = bias_files,region=region, overwrite='yes')\n",
    "\n",
    "results_trim = step.run()\n",
    "\n",
    "step = deviation_calculation(wdir = bias_dir, files = results_trim, overwrite='yes')\n",
    "\n",
    "results_dc = step.run()\n",
    "\n",
    "step = gain_correction(wdir = bias_dir, files = results_dc,overwrite='yes')\n",
    "\n",
    "results_gc = step.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ccbb9-8819-4f4e-a841-3730233e20bf",
   "metadata": {},
   "source": [
    "The masterbias can be calculated with the $image\\_combination$ class. The methods for combining images are median, average, sum, scaled and weighted (please refer to [the ccdproc image combination guide](https://ccdproc.readthedocs.io/en/latest/image_combination.html) for more information). The methods for clipping method are sigclip, extrema_clipping and minmax. You may also specify the maximum amount of memory used, the deafult is 1 GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f14a8-8d03-4cb3-921b-bb8413f866c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterbias = image_combination(wdir=bias_dir,files=results_gc,output_name='Masterbias.fits',\n",
    "                               combining_method='median',clipping_method='sigclip',minclip=2,maxclip=5).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817e97a-0023-4242-a8d4-167ab62a0afe",
   "metadata": {},
   "source": [
    "Let's repeat the same steps but for flats, in this case we will subtract the masterbias file and combine the images into their respective masterflatfield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d90c6-716f-4e04-b6d6-556813ccd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(flat_dir)):\n",
    "\n",
    "    step = trimmer(wdir = flat_dir[ii], files = flat_files[ii], region=region, overwrite='yes')\n",
    "\n",
    "    results_trim = step.run()\n",
    "\n",
    "    step = deviation_calculation(wdir = flat_dir[ii], files = results_trim, overwrite='yes')\n",
    "\n",
    "    results_dc = step.run()\n",
    "\n",
    "    step = gain_correction(wdir = flat_dir[ii], files = results_dc,overwrite='yes')\n",
    "\n",
    "    results_gc = step.run()\n",
    "\n",
    "    results_bias_subtraction = subtract_bias(wdir=flat_dir[ii],files=results_gc,masterbias_file=masterbias,overwrite='yes').run()\n",
    "\n",
    "    globals()['masterflat_' + flat_filters[ii]] = image_combination(wdir=flat_dir[ii],\n",
    "                                                                    files=results_bias_subtraction,\n",
    "                                                                    output_name=f'Masterflat_{flat_filters[ii]}.fits',\n",
    "                                                                    combining_method='median',clipping_method='sigclip',\n",
    "                                                                    minclip=2,maxclip=5).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33a0e8-2df0-452e-8c0b-8130fb37af1e",
   "metadata": {},
   "source": [
    "Now let's do the same for the object files, we will divide by the master flatfield, there is no need to normalize as the function takes care of that by itself. Unlike in the photometry case we are interested in eliminating the cosmic rays, there are two methods, cosmic_ray_laplacian_correction and cosmic_ray_median_correction, the latter is similar to the one available in IRAF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a032c0-4bbc-41ac-97ce-3810eb714ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(object_dir)):\n",
    "\n",
    "    step = trimmer(wdir = object_dir[ii], files = object_files[ii], region=region, overwrite='yes')\n",
    "\n",
    "    results_trim = step.run()\n",
    "\n",
    "    step = deviation_calculation(wdir = object_dir[ii], files = results_trim, overwrite='yes')\n",
    "\n",
    "    results_dc = step.run()\n",
    "\n",
    "    step = gain_correction(wdir = object_dir[ii], files = results_dc,overwrite='yes')\n",
    "\n",
    "    results_gc = step.run()\n",
    "\n",
    "    step = cosmic_ray_laplacian_correction(wdir = object_dir[ii], files = results_gc, overwrite='yes')\n",
    "\n",
    "    results_crlc = step.run()\n",
    "\n",
    "    results_bias_subtraction = subtract_bias(wdir=object_dir[ii],files=results_crlc,masterbias_file=masterbias,overwrite='yes').run()\n",
    "\n",
    "    results_flat_correction = correct_flatfield(wdir=object_dir[ii], files=results_bias_subtraction, \n",
    "                                                masterflatfield_file = globals()['masterflat_' + flat_filters[ii]], overwrite='yes').run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff8e29-6b42-45b2-8626-a5f2994a8876",
   "metadata": {},
   "source": [
    "Done! The design philosophy behind this pipeline is that you can either run it entirely with minimal user input (only at the beginning) or step by step. This generates a lot of intermediate files, and these may not be necessary at the end. So let's just delete them and finalize stage 1. The finalize_stage1 class returns the directories and files that have been reduced and rearranges the files so that you have date/target/filters/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4fb2c-50ea-4da8-a59d-b4517cc1bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories, files = finalize_stage1(data_dir=data_dir,object_dir=object_dir,bias_dir=bias_dir,flatfield_dir=flat_dir).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f44b4-2b5d-40b8-a52e-765dd65cb26d",
   "metadata": {},
   "source": [
    "### Second stage: Astrometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bae1d2-6930-46dc-aedf-735b65ae7757",
   "metadata": {},
   "source": [
    "The final objective is doing astrophotography, so, aligning images is a must. This program makes use of [Astrometry.net](https://nova.astrometry.net/) to astromtrize images, they will later be projected in the same wcs. You should register and the obtain your API key. It will be used to astrometrize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d237a-2abf-4a52-b132-2eee345643c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "fwhms_out = []\n",
    "for ii in range(len(directories)):\n",
    "    globals()[f'failed_astrometry_files_filter_{object_filters[ii]}'] = []\n",
    "    fwhms = get_fwhm(files[ii]).estimate_fwhm()\n",
    "    fwhms_out.append(fwhms)\n",
    "    print(fwhms)\n",
    "    globals()[f'failed_astrometry_files_filter_{object_filters[ii]}'].append(astrometry(directory=directories[ii],\n",
    "                                                                                        files=files[ii],api_key=api_key,fwhm=fwhms).run())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a7949-09d5-4248-8795-b8b2d27bbe2f",
   "metadata": {},
   "source": [
    "If you decided to stop here (it can take up to 2 hours in the worst case scenario depending on the astrometry) and continue another day but stll decided to continue with this code you would find that the lists of files etc are not stored in memory. Because doing it yourself is quite annoying the code has got you covered. Just run the following. Data dir should be where the data are, so /reduced_data/YYMMDD/target/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398dea7-3e80-44d2-9167-d5bbdeab664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories, files, filters = get_directory(data_dir = '/XXXX/XXXX/').science_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7d761-f044-452b-914b-a81271627640",
   "metadata": {},
   "source": [
    "### Third stage: Image combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a079e-29c4-4cb1-b3a0-fcf6c6601ae1",
   "metadata": {},
   "source": [
    "We are ready to align and combine the images to create our beautiful color images. First let's align all images with respect to one reference frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b6cd9-ce97-4edd-aaed-f89492d696ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_images = align(files_to_align=files,directories=directories,reference_frame=files[0][0]).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee0ae7-1ad4-4189-a35a-304d75bc47ff",
   "metadata": {},
   "source": [
    "Let's combine those images which are in the same filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b630e-66fa-40a6-8f3d-e391f5674097",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_images = []\n",
    "for ii in range(len(filters)):\n",
    "    combined_images.append(image_combination(wdir=directories[ii],files=aligned_images[ii],\n",
    "                                             output_name=f'Combined_image_{filters[ii]}.fits',\n",
    "                                             combining_method='median',clipping_method='sigclip',\n",
    "                                             minclip=2,maxclip=5,remove_files='yes').run())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebaf441-6586-400b-aa0a-26cf1293e4ee",
   "metadata": {},
   "source": [
    "Done!! This is the end of stage 3 and the astrophotography guide. You can now create beautiful images, good luck!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d663ffd-3840-4eda-bc83-7c4635c9cce6",
   "metadata": {},
   "source": [
    "# End!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
